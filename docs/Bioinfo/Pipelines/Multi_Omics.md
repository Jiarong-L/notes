
特征的提取，就是各种矩阵分解 (Supervised / Unsupervised)、试图解释各种变异（Variance），不同方法只是优化目标不同罢了


| 多组学的整合时机 | -- | -- |
| -- | -- | -- |
| Early | 直接将各组学的原始数据合并为矩阵，然后再降维、分析 | 保留所有信息，但数据维度、尺度、噪声水平不同，需谨慎处理 （适合DL？） |
| Intermediate | 降维、提取各组学的代表性特征（PC），再进行整合分析 |  |
|  | **联合降维(JDR)** 重建 latent space（假设所有组学数据共享隐变量/某种特征） | 常用、稳健 |
| Late | 各组学独立分析，比较最终结果（寻找共识/相关性） | 忽略分子层之间的相互作用 |



## Early Integration - Supervised

即，投影空间必须包括希望预测的信息

最简单的方法，即训练一个分类 DL 网络，其中间层输出的 Embeddings 就是降维后的向量

```bash
         RNA -->  ...
         DNA -->  ... 
Metabolomics -->  ...    merge -->  Embedding  -->  Perdict (...)
        Envs -->  ...
         ... -->  ...
```


## Intermediate Integration - 联合降维(JDR)

1. 输入：各组学的矩阵 ```X1  X2 X3 ...```，行（样本）对齐、但列（原始特征）不同

2. 处理：找到一个数学变换（一组变换向量），将所有高维矩阵映射到同一个低维空间（Latent Space）
    - 捕获所有组学数据中共有的变异，而过滤掉各数据集特有的噪声和无关变异

3. 输出
    - 新低维空间中的坐标 --- Score Matrix / 样本在各维度上的值（潜在因子 Joint Latent Factors）
    - 原始特征对每个潜在因子的贡献程度（载荷 Loadings）


(可能bias向高维的组学数据)



### CCA 相关扩展

参考 [Ecology - Canonical Analysis](../../Statistics/Ecology.md#canonical-analysis)，CCA/RDA 的目标是寻找一组变换向量 ```u, v``` 使得两个数据集的投影 ```Xu, Yv``` 的相关性最大化，其优化目标：Maximize(相关系数 ρ)

而广义典型相关分析 (GCCA) 的目标，从两个数据集、扩展为使得多个数据集的投影 ```Xu, Yv, Zw, ...``` 之间的（广义）相关性最大化，其优化目标：Maximize(方差)，而方差意味着所有成对投影相关性的总和 ----- 或计算所有投影与 共识向量```z``` 的方差之和

比较常用的扩展有 Regularized GCCA (RGCCA)，引入正则化（L2惩罚）来处理高维数据（特征数 >> 样本数）的共线性问题

Sparse GCCA 引入稀疏性惩罚（L1惩罚），使得载荷向量中许多系数为0，更易识别出贡献最大的关键特征（基因、代谢物等）


### 多组学因子分析 (MOFA/MOFA+)

假设所观察到的多个组学数据集都是由一组共同的、未被观测到的潜在因子（Latent Factors） 所驱动的，于是这一组因子可以共同解释所有组学数据中大部分的变异。MOFA/MOFA+ 希望找到这一组因子

```bash

观测数据X = （ 潜在因子Z × 权重W ） +  τ1 + τ2 + τ3 ...

MOFA+的目标是找到真实后验分布 P(Z, W, τ... | X) 的一个最佳近似分布 Q(Z, W, τ...)
即：推断 潜在因子Z 、 权重W 的后验分布

1. 已知先验τ
    噪声 -- 可以自行训练：高斯模型用于连续数据，伯努利模型用于二分类数据
    组学特异性偏移 -- 先验知识，e.g.符合某种分布

2. 需指定潜在因子的数量（通常可以先设一个稍大的值，再加入稀疏惩罚）

3. 变分贝叶斯推断（VB）通常引入均值场假设（Mean-Field Assumption），将联合近似分布进行简化：

    Q(Z, W, τ...) = q(Z)q(W)q(τ...)      i.e. 假设相互独立

4. 坐标上升变分推断（CAVI）算法，直到目标函数收敛
    每一次：固定其它所有，只更新 Z / W / τ... 中的一个，依次轮回

注意：VB得到的是分布（通常是高斯分布 N[mean, variance]），而非单个值
```

至于这些因子代表了什么，需要依据其与原始特征的关联进行推断（已知/未知的生物学注释 pathway/GO/network 、批次效应、环境混杂因素、...），MOFA是无监督的降维

MOFA 使用期望最大化（EM）算法，MOFA+ 使用变分贝叶斯推断（VB）。后者速度更快，且提供 R/Python 包，更推荐使用

其它：iClusterBayes 使用马尔可夫链蒙特卡洛（MCMC） 方法进行后验推理


### 联合非负矩阵分解 (jNMF)


非负矩阵分解 V = WH 由于其非负值，更容易进行生物学性质解读

jNMF 希望找到一个共同的 H矩阵（pattern matrices），对于每一个组学矩阵的分解，都有 ```X1 ≈ W1 * H,   X2 ≈ W2 * H,   ...```

强制共享的 H 矩阵直接提供了样本的联合低维表示



### 其它 Unspervised

* AE (Encoder-Decoder) 中，每个组学的 Encoder 可将其映射到同一个共享的潜在空间，Decoder 可**相互**重建各自的原始数据

* 多重协同惯性分析（MCIA）：寻找一个低维空间（轴），这个轴应代表了所有样本某种共同的的变化趋势、故其上单位向量称为共识向量 ```ui``` ，希望各组学的投影 ```Xk vk```与它保存正向的线性关系（协方差之和最大化）
    - 依旧是固定一个参数，交替更新 ```ui``` / ```vk``` 
    - 协方差的意义：不同组学对同一样本的投影尽可能相似

* JIVE (Joint and Individual Variation Explanation)

```bash
假设有多个组学数据 X_1, X_2, X_3, ..., X_k，每个组学的 JIVE 的模型为：

    X_k = J_k + A_k + E_k

J --- 共享的变异模式 (同时发生在多个水平的调控)
A --- 组学独有的变异模式 (不同水平的调控、技术噪声)
E --- 残差噪声 (Residual Noise)

（A与J正交）

JIVE 的目标是将原始数据矩阵精确地分解为这三部分之和：固定其一、交替更新J与A

对 J或A 进行SVD分解，可以得到 Joint/Individual 的 Scores（样本低维坐标）、Loadings
```


### Supervised

无监督 JDR 寻找共享变异（"方差最大的方向"），**有监督** JDR 寻找**与类别标签相关的**共享变异（"与类别标签Y协方差最大的方向"）

常用 DIABLO (多组学PLS-DA)：

```bash
PLS-DA 的优化目标：寻找数据 X 的投影 Xw，使之与类别标签 Y 的相关性尽可能大，即
    寻找合适的向量 w 和 c，使得 Cov(Xw,Yc) 最大化

-----------------------------------------------------
依次计算第 i 个 t :
    Loop 直到 w 收敛:
        1)  t = Xw                “潜在成分”
        1)  c = Y.转置 * t         “关系强度”
        2)  w = X.转置 * Yc        “更新w”
        3)  w = w / ||w||
    
    p = X.转置 * t / (t.转置 * t)       "Loading for X"
    X = X - t * p.转置                  "从X中减去当前成分的信息"
    Y = Y - t * c.转置                  "从Y中减去当前成分的信息"
-----------------------------------------------------

合并i个结果，最终得到：
      自变量矩阵 X = TP.转置 + Residual1
    响应变量矩阵 Y = TC.转置 + Residual2

T --- 共享的 Score Matrix，即样本在新空间中的投影坐标
P --- Loading Matrix for X
C --- Loading Matrix for Y

-----------------------------------------------------

多组学PLS-DA中则有多个自变量矩阵 X1,X2,... 和对应的多个 Loading P1,P2,...
优化目标：
    1. 各自 Cov({Xi},Yc) 最大化
    2. 各组学的投影 {Xi}{wi} 彼此之间相关性最大化
```


## Intermediate Integration - Similarity Network Fusion (SNF)

工具：[SNFtool](https://github.com/maxconway/SNFtool)


1. 为每个组学构建相似性网络：Node (样本)、Edge (计算特征向量间的欧氏距离、只保留k个最近邻、0/1)
    - ```S_k```

2. 标准化：令矩阵每一行之和为1（divided by rowsum/此样本的度），则元素可被解释为随机游走的转移概率
    - ```P_k```

3. 融合、迭代直至 ```P``` 收敛
    - ```P_k' = S_k × WeightedMean(P_i where i ≠ k) × S_k.T```





